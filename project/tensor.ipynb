{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor manipulation: basics\n",
    "\n",
    "Contracting tensors together forms the core of the Psi4NumPy project. First let us consider the popluar [Einstein Summation Notation](https://en.wikipedia.org/wiki/Einstein_notation) which allows for very succinct descriptions of a given tensor contraction.\n",
    "\n",
    "For example, let us consider a [inner (dot) product](https://en.wikipedia.org/wiki/Dot_product):\n",
    "$$c = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "With the Einstein convention, all indices that are repeated are considered summed over, and the explicit summation symbol is dropped:\n",
    "$$c = A_{ij} * B_{ij}$$\n",
    "\n",
    "This can be extended to [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication):\n",
    "\\begin{align}\n",
    "\\rm{Conventional}\\;\\;\\;  C_{ik} &= \\sum_{j} A_{ij} * B_{jk} \\\\\n",
    "\\rm{Einstein}\\;\\;\\;  C &= A_{ij} * B_{jk} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where the $C$ matrix has *implied* indices of $C_{ik}$ as the only repeated index is $j$.\n",
    "\n",
    "However, there are many cases where this notation fails. Thus we often use the generalized Einstein convention. To demonstrate let us examine a [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)):\n",
    "$$C_{ij} = \\sum_{ij} A_{ij} * B_{ij}$$\n",
    "\n",
    "This operation is nearly identical to the dot product above, and is not able to be written in pure Einstein convention. The generalized convention allows for the use of indices on the left hand side of the equation:\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "Usually it should be apparent within the context the exact meaning of a given expression.\n",
    "\n",
    "Finally we also make use of Matrix notation:\n",
    "\\begin{align}\n",
    "{\\rm Matrix}\\;\\;\\;  \\bf{D} &= \\bf{A B C} \\\\\n",
    "{\\rm Einstein}\\;\\;\\;  D_{il} &= A_{ij} B_{jk} C_{kl}\n",
    "\\end{align}\n",
    "\n",
    "Note that this notation is signified by the use of bold characters to denote matrices and consecutive matrices next to each other imply a chain of matrix multiplications! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einsum\n",
    "\n",
    "To perform most operations we turn to [NumPy's einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) which allows the Einsten convention as an input. In addition to being much easier to read, manipulate, and change, it is also much more efficient that a pure Python implementation. To begin let us consider the construction of the following tensor (which you may recognize):\n",
    "$$G_{pq} = 2.0 * I_{pqrs} D_{rs} - 1.0 * I_{prqs} D_{rs}$$ \n",
    "\n",
    "First let us import our normal suite of modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psi4\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use conventional Python loops and einsum to perform the same task. Keep size relatively small as these 4-index tensors grow very quickly in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The loop and einsum fock builds match:    True\n\nTime for loop G build:           0.5851 seconds\nTime for einsum G build:         0.0007 seconds\nG builds with einsum are 804.9164 times faster than Python loops!\n"
     ]
    }
   ],
   "source": [
    "size = 20\n",
    "\n",
    "if size > 30:\n",
    "    raise Exception(\"Size must be smaller than 30.\")\n",
    "D = np.random.rand(size, size)\n",
    "I = np.random.rand(size, size, size, size)\n",
    "\n",
    "# Build the fock matrix using loops, while keeping track of time\n",
    "tstart_loop = time.time()\n",
    "Gloop = np.zeros((size, size))\n",
    "for p in range(size):\n",
    "    for q in range(size):\n",
    "        for r in range(size):\n",
    "            for s in range(size):\n",
    "                Gloop[p, q] += 2 * I[p, q, r, s] * D[r, s]\n",
    "                Gloop[p, q] -=     I[p, r, q, s] * D[r, s]\n",
    "\n",
    "g_loop_time = time.time() - tstart_loop\n",
    "\n",
    "# Build the fock matrix using einsum, while keeping track of time\n",
    "tstart_einsum = time.time()\n",
    "J = np.einsum('pqrs,rs', I, D, optimize=True)\n",
    "K = np.einsum('prqs,rs', I, D, optimize=True)\n",
    "G = 2 * J - K\n",
    "\n",
    "einsum_time = time.time() - tstart_einsum\n",
    "\n",
    "# Make sure the correct answer is obtained\n",
    "print('The loop and einsum fock builds match:    %s\\n' % np.allclose(G, Gloop))\n",
    "# Print out relative times for explicit loop vs einsum Fock builds\n",
    "print('Time for loop G build:   %14.4f seconds' % g_loop_time)\n",
    "print('Time for einsum G build: %14.4f seconds' % einsum_time)\n",
    "print('G builds with einsum are {:3.4f} times faster than Python loops!'.format(g_loop_time / einsum_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the einsum function is considerably faster than the pure Python loops and much cleaner and easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot\n",
    "\n",
    "Now let us turn our attention to a more canonical matrix multiplication example such as:\n",
    "$$D_{il} = A_{ij} B_{jk} C_{kl}$$\n",
    "\n",
    "We could perform this operation using einsum; however, matrix multiplication is an extremely common operation in all branches of linear algebra. Thus, these functions have been optimized to be more efficient than the `einsum` function. The matrix product will explicitly compute the following operation:\n",
    "$$C_{ij} = A_{ij} * B_{ij}$$\n",
    "\n",
    "This can be called with [NumPy's dot function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html#numpy.dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pair product allclose: True\n"
     ]
    }
   ],
   "source": [
    "size = 200\n",
    "A = np.random.rand(size, size)\n",
    "B = np.random.rand(size, size)\n",
    "C = np.random.rand(size, size)\n",
    "\n",
    "# First compute the pair product\n",
    "tmp_dot = np.dot(A, B)\n",
    "tmp_einsum = np.einsum('ij,jk->ik', A, B, optimize=True)\n",
    "print(\"Pair product allclose: %s\" % np.allclose(tmp_dot, tmp_einsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have proved exactly what the dot product does, let us consider the full chain and do a timing comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chain multiplication allclose: True\n",
      "\n",
      "np.dot time:\n",
      "5.21 ms ± 28.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "np.einsum time\n",
      "1.31 s ± 11.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "D_dot = np.dot(A, B).dot(C)\n",
    "D_einsum = np.einsum('ij,jk,kl->il', A, B, C, optimize=True)\n",
    "print(\"Chain multiplication allclose: %s\" % np.allclose(D_dot, D_einsum))\n",
    "\n",
    "print(\"\\nnp.dot time:\")\n",
    "%timeit np.dot(A, B).dot(C)\n",
    "\n",
    "print(\"\\nnp.einsum time\")\n",
    "# no optimization here for illustrative purposes!\n",
    "%timeit np.einsum('ij,jk,kl->il', A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the `np.dot` times are roughly ~2,000 times faster. The reason is twofold:\n",
    " - The `np.dot` routines typically call [Basic Linear Algebra Subprograms (BLAS)](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms). The BLAS routines are highly optimized and threaded versions of the code.\n",
    " - The `np.einsum` code will not factorize the operation by default; Thus, the overall cost is ${\\cal O}(N^4)$ (as there are four indices) rather than the factored $(\\bf{A B}) \\bf{C}$ which runs ${\\cal O}(N^3)$.\n",
    " \n",
    "The first issue is difficult to overcome; however, the second issue can be resolved by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "np.einsum factorized time:\n",
      "4.94 ms ± 31.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"np.einsum factorized time:\")\n",
    "# no optimization here for illustrative purposes!\n",
    "%timeit np.einsum('ik,kl->il', np.einsum('ij,jk->ik', A, B), C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On most machines the factorized `einsum` expression is only ~10 times slower than `np.dot`. While a massive improvement, this is a clear demonstration the BLAS usage is usually recommended. It is a tradeoff between speed and readability. The Psi4NumPy project tends to lean toward `einsum` usage except in case where the benefit is too large to pass up.\n",
    "\n",
    "Starting in NumPy 1.12, the [einsum function](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) has a `optimize` flag which will automatically factorize the einsum code for you using a greedy algorithm, leading to considerable speedups at almost no cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "np.einsum optimized time\n",
      "10.9 ms ± 462 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nnp.einsum optimized time\")\n",
    "%timeit np.einsum('ij,jk,kl->il', A, B, C, optimize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, using `optimize=True` for automatic factorization is only 25% slower than `np.dot`. Furthermore, it is ~5 times faster than factorizing the expression by hand, which represents a very good trade-off between speed and readability. When unsure, `optimize=True` is strongly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e4c6ce54e6d1ccff551279c9aafc06b78c48fd9e60d6b4e74c0583a74ec1d1f9"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}